<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Beneath Abstraction</title><link>https://www.beneathabstraction.com/tags/llm/</link><description>Recent content in LLM on Beneath Abstraction</description><generator>Hugo</generator><language>en-au</language><lastBuildDate>Sat, 22 Jun 2024 02:26:59 +0000</lastBuildDate><atom:link href="https://www.beneathabstraction.com/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Enhancing Language Models with RAG Architecture</title><link>https://www.beneathabstraction.com/post/azureaistudioragarchitecture/</link><pubDate>Fri, 21 Jun 2024 18:50:46 +1000</pubDate><guid>https://www.beneathabstraction.com/post/azureaistudioragarchitecture/</guid><description>Enhancing Language Models with RAG Architecture Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) capabilities, like those of GPTs, by integrating an information retrieval system. This addition grounds data and controls the context for the LLM’s response generation.
From Ungrounded to Grounded Prompts A pre-trained language model uses its training data to answer prompts. However, this data might not align with the prompt’s context. Grounding a prompt with relevant data transforms how a language model responds, making them more contextualized and accurate.</description></item></channel></rss>